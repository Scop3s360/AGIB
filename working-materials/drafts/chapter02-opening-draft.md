# Chapter 2: The Landscape of AGI Beliefs - Opening Section (DRAFT)

You're talking to a friend about AI. They've just read another article about existential risk, and they're worried. "Do you think AI will take over the world?"

You pause. Because the honest answer is complicated.

If they mean "Will current AI systems suddenly become autonomous agents with goals of their own and decide to overthrow humanity?" - No. That's not how any of this works. Current AI has no goals, no desires, no motivation to do anything. It's a tool. A very sophisticated tool, but a tool nonetheless. It can't "want" to take over any more than your calculator can want to solve equations.

But if they mean "Could we build something that has genuine goals, genuine autonomy, genuine agency - and could that thing's goals conflict with ours?" - Maybe. But that would require building something fundamentally different. Not just a smarter version of what we have now. We'd have to create artificial life - a being with genuine psychology, emotions, motivations. Something that wants things.

And here's where it gets tricky: you can't explain this distinction without first explaining that there are two completely different things people mean when they say "AGI." And that most conversations about AI risk are confused because people are imagining different futures, using the same words.

Your friend is still waiting for an answer. You realize you can't give a simple yes or no, because the question itself is built on assumptions that need unpacking.

## The Confusion Problem

This isn't just one confused conversation. It's every conversation about AI. We're having urgent debates about AI's future - investing trillions of dollars, making policy decisions, worrying about existential risk, arguing about regulation - but we're not even working from the same map.

Ask ten people what they think about AGI and you'll get ten different answers that aren't even addressing the same question:

"It's five years away." "It's impossible." "We already have it." "We'll never have it." "It will save us." "It will kill us." "It's just statistics." "It's already conscious." "We need to slow down." "We need to accelerate."

These aren't just different predictions. They're different frameworks, different assumptions about what intelligence is, what's possible, what matters. And because we're using the same words to mean different things, we talk past each other. The scaling optimist and the architecture skeptic aren't disagreeing about facts - they're operating from incompatible models of what intelligence requires. The person worried about AI takeover and the person dismissing those concerns as science fiction aren't evaluating the same scenario - they're imagining completely different futures.

This confusion isn't academic. It has consequences.

Policy makers are trying to regulate something they don't understand, based on advice from experts who disagree about fundamentals. Investors are pouring money into companies based on hype that conflates narrow AI capabilities with progress toward AGI. Researchers are pursuing different goals under the same label, some trying to build better tools, others inadvertently working toward artificial life. The public oscillates between fear and complacency, neither response calibrated to actual risks.

And underneath all of it, there's a deeper confusion: we don't actually know what we're trying to build. Or rather, different people are trying to build different things, and we haven't acknowledged that these are different things with radically different implications.

Before we can talk productively about AGI - whether it's possible, how to build it, what risks it poses, what we owe it - we need to map the territory. What do people actually believe? What are developers seeing? What are researchers dreaming of? And why is everyone talking past each other?

## The Map We Need

This chapter is that map. Not a comprehensive survey of every position - that would take volumes - but a sketch of the major camps, the key disagreements, and the gaps between perception and reality.

We'll look at what the public believes, and why those beliefs are both understandable and misleading. We'll examine what AI developers actually see when they work with these systems - the scaling optimists who think we're on the right track, the architecture skeptics who think we're missing fundamental pieces, the alignment researchers who think safety is the bottleneck, the pragmatists who just want to build useful tools.

We'll explore what researchers dream of - not just what they say they're building, but what drives them. The dream of Cognitive AGI: intelligence without consciousness, capability without complications. The dream of Psychological AGI: genuine artificial life, beings that think and feel. The dream of threading the needle between the two. And the unspoken dreams that shape the field more than anyone admits.

We'll identify the gaps between perception and reality - where marketing diverges from capability, where impressive demos hide systematic limitations, where progress in narrow domains gets mistaken for progress toward general intelligence.

And we'll see why clarity matters. Not just for intellectual satisfaction, but because the decisions we make now - about research priorities, safety measures, regulations, and whether to proceed at all - depend on understanding what we're actually building.

By the end of this chapter, you'll understand why your friend's question - "Will AI take over the world?" - doesn't have a simple answer. Not because the answer is uncertain, but because the question itself contains assumptions that need unpacking. The answer depends entirely on what kind of AI we build. And that's a choice we're making right now, mostly without realizing it.

## What the Public Believes

Let's start with the most common beliefs about AI, the ones you encounter in casual conversation, in news articles, in social media debates. These beliefs aren't stupid - they're reasonable responses to confusing information. But they're also preventing clear thinking about what's actually happening and what's actually possible.

### The Binary Trap

The public conversation about AGI has collapsed into binaries. It's either imminent or impossible. It will either save humanity or destroy it. Current AI is either "just statistics" or already sentient. There's no middle ground, no nuance, no room for "it depends."

This binary thinking makes sense psychologically. Humans are pattern-matching creatures, and we like clear categories. Ambiguity is uncomfortable. So when faced with something as complex and uncertain as AI, we simplify. We pick a side. We commit to a narrative.

But reality doesn't fit into binaries. AGI might be achievable but require fundamental breakthroughs we haven't conceived yet - neither five years away nor impossible. Current AI might be genuinely impressive at specific tasks while completely lacking general intelligence - neither "just statistics" nor sentient. The future might hold both tremendous benefits and serious risks, with the balance depending on choices we haven't made yet.

The binary trap prevents productive conversation. If you believe AGI is imminent, you dismiss concerns about fundamental limitations as Luddite skepticism. If you believe it's impossible, you dismiss impressive capabilities as parlor tricks. If you believe AI will save us, you downplay risks. If you believe it will kill us, you miss opportunities. Neither extreme engages with the actual complexity.

### The Anthropomorphization Instinct

When AI writes a poem, people assume it understands emotion. When it passes the bar exam, they assume it understands law. When it apologizes, they assume it feels remorse. This isn't irrational - it's how we're wired.

Humans have a deeply ingrained tendency to attribute minds to things that behave in mind-like ways. This served us well evolutionarily - it's better to mistakenly see agency in a rustling bush than to miss the predator that's actually there. We err on the side of seeing minds, and w